{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import usual libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 1 - DATA MANIPULATION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 2 - DATA VISUALISATION\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 3 - STATISTICS\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "# 4 - MACHINE LEARNING\n",
    "## 4.1 - Preprocessing\n",
    "\n",
    "### 4.1.1 - Scalers\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "### 4.1.2 - Encoders\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "### 4.1.3 - Crossvalidation, Training, Model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "### 4.1.4 - Evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics  import ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from math import sqrt\n",
    "\n",
    "# Make all figures tiny for readability purpose\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (5,3)\n",
    "import macrobond_data_api as mda\n",
    "from macrobond_data_api.web import WebClient\n",
    "\n",
    "from macrobond_data_api.web import WebClient\n",
    "from macrobond_data_api.common.types import SearchFilter\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = 'CPIAUCSL'\n",
    "Drop = ['value_x','CPIAUCSL','core_cpi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.read_csv('fbd_df_w_fred_gdp.csv', index_col='Unnamed: 0', parse_dates=True)\n",
    "X = total_df.drop(columns=Drop)[:-21] # (8885, 651)  1990-01-01 to 2024-01-31 8871 non-null \n",
    "y = total_df[Target].values[:-21]\n",
    "new_data = total_df.drop(columns=Drop).iloc[-1:]\n",
    "y= y[-X.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Calculate the mean excluding NaN values\n",
    "mean_val = np.nanmean(y)\n",
    "# Replace NaN values with the mean\n",
    "y[np.isnan(y)] = mean_val\n",
    "y = pd.Series(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non Linear PCA\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Updated preprocessing pipeline with Kernel PCA\n",
    "# preproc = make_pipeline(\n",
    "#     SimpleImputer(strategy='mean'),\n",
    "#     StandardScaler(),\n",
    "#     KernelPCA(n_components=None, kernel='rbf')  # Example with RBF kernel do not enter the % \n",
    "# ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear PCA \n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Adjusted preprocessing pipeline with named steps\n",
    "preproc = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95, random_state=42))  # Named step 'pca'\n",
    "])\n",
    "\n",
    "\n",
    "# # Feature selection remains the same\n",
    "# # preproc_selector = Pipeline([\n",
    "# #     ('preprocessing', preproc),  # Include the preprocessing steps with PCA\n",
    "# #     ('feature_selection', SelectPercentile(\n",
    "# #         mutual_info_regression,\n",
    "# #         percentile=90 # Keep 90% of all features\n",
    "# #     ))\n",
    "# # ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = XGBRegressor(random_state=42)\n",
    "\n",
    "# param_distributions = {\n",
    "#     'model__n_estimators': [200],  \n",
    "#     'model__learning_rate': [0.1],  \n",
    "#     'model__max_depth': [16, 20],  \n",
    "# }\n",
    "\n",
    "# pipe = Pipeline([\n",
    "#     ('preprocessor', preproc),\n",
    "#     ('model', model)\n",
    "# ])\n",
    "\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     pipe,\n",
    "#     param_distributions=param_distributions,\n",
    "#     n_iter=100,  # Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.\n",
    "#     scoring='neg_mean_absolute_error',  # Assuming MSE is the metric of interest; adjust as needed.\n",
    "#     cv=5,\n",
    "#     verbose=2,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1  # Use all available cores\n",
    "# )\n",
    "\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters found: \", random_search.best_params_)\n",
    "# print(\"Best score found: \", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1076653324736503\n",
      "-1.524087226392913\n"
     ]
    }
   ],
   "source": [
    "# genrally higher N_estimators and lower learning rates are more robust \n",
    "# Best parameters found:  {'model__n_estimators': 150, 'model__max_depth': 6, 'model__learning_rate': 0.1}\n",
    "# Best score found:  -0.10133019738065827\n",
    "\n",
    "# Defining the best model\n",
    "model_best =  XGBRegressor(n_estimators=200, max_depth = 20, learning_rate = 0.1, random_state=42)\n",
    "\n",
    "# Creating the pipeline with memory caching\n",
    "pipe_best = make_pipeline(preproc, model_best)\n",
    "\n",
    "# Assuming X_train and y_train are already defined and contain your training data\n",
    "score = cross_val_score(pipe_best, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Printing the standard deviation and mean of the cross-validation scores\n",
    "print(score.std())\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_best.fit(X_train,y_train)\n",
    "y_pred = pd.Series(pipe_best.predict(X_test))\n",
    "y_test = pd.Series(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def forecast_accuracy(y_pred: pd.Series, y_true: pd.Series) -> float:\n",
    "\n",
    "    mape = np.mean(np.abs(y_pred - y_true)/np.abs(y_true))  # Mean Absolute Percentage Error\n",
    "    me = np.mean(y_pred - y_true)             # ME\n",
    "    mae = np.mean(np.abs(y_pred - y_true))    # MAE\n",
    "    mpe = np.mean((y_pred - y_true)/y_true)   # MPE\n",
    "    rmse = np.mean((y_pred - y_true)**2)**.5  # RMSE\n",
    "    corr = np.corrcoef(y_pred, y_true)[0,1]   # Correlation between the Actual and the Forecast\n",
    "    mins = np.amin(np.hstack([y_pred.values.reshape(-1,1), y_true.values.reshape(-1,1)]), axis=1)\n",
    "    maxs = np.amax(np.hstack([y_pred.values.reshape(-1,1), y_true.values.reshape(-1,1)]), axis=1)\n",
    "    minmax = 1 - np.mean(mins/maxs)             # minmax\n",
    "    acf1 = acf(y_pred-y_true, fft=False)[1]                      # Lag 1 Autocorrelation of Error\n",
    "\n",
    "    forecast = ({\n",
    "        'mape':mape,\n",
    "        'me':me,\n",
    "        'mae': mae,\n",
    "        'mpe': mpe,\n",
    "        'rmse':rmse,\n",
    "        'acf1':acf1,\n",
    "        'corr':corr,\n",
    "        'minmax':minmax\n",
    "    })\n",
    "\n",
    "    return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mape': 0.32115330375230644,\n",
       " 'me': 45.047304088586486,\n",
       " 'mae': 46.44604239935349,\n",
       " 'mpe': 0.31193208262630406,\n",
       " 'rmse': 53.81115218354335,\n",
       " 'acf1': nan,\n",
       " 'corr': 0.9992562418763288,\n",
       " 'minmax': 0.005871602262185416}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_accuracy(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: 285.64145\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction with the new observation\n",
    "prediction = pipe_best.predict(new_data)[0]\n",
    "\n",
    "print(\"Predicted value:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198.456528</td>\n",
       "      <td>199.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162.364014</td>\n",
       "      <td>162.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152.673340</td>\n",
       "      <td>150.512903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168.298355</td>\n",
       "      <td>164.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254.435532</td>\n",
       "      <td>254.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>207.234177</td>\n",
       "      <td>207.454049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>182.271179</td>\n",
       "      <td>181.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>252.233490</td>\n",
       "      <td>251.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>165.953003</td>\n",
       "      <td>165.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>192.072418</td>\n",
       "      <td>191.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        y_pred      y_test\n",
       "0   198.456528  199.700000\n",
       "1   162.364014  162.800000\n",
       "2   152.673340  150.512903\n",
       "3   168.298355  164.700000\n",
       "4   254.435532  254.277000\n",
       "..         ...         ...\n",
       "73  207.234177  207.454049\n",
       "74  182.271179  181.200000\n",
       "75  252.233490  251.663000\n",
       "76  165.953003  165.900000\n",
       "77  192.072418  191.700000\n",
       "\n",
       "[78 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_forecast = pd.DataFrame()\n",
    "\n",
    "# Assign rounded values of y_pred and y_test to the DataFrame\n",
    "model_forecast['y_pred'] = y_pred.values\n",
    "model_forecast['y_test'] = y_test.values\n",
    "\n",
    "# Display the DataFrame\n",
    "model_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer()), (&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;pca&#x27;, PCA(n_components=0.95, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer()), (&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;pca&#x27;, PCA(n_components=0.95, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.95, random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
       "                ('pca', PCA(n_components=0.95, random_state=42))])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming X_train and y_train are your features and target variables respectively\n",
    "preproc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessing pipeline and transform X_train\n",
    "X_train_transformed = preproc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Feature_0  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  \\\n",
      "0   -15.580499   2.693344   0.288657  -7.202345   1.155502  -0.581511   \n",
      "1    -4.324043  -0.874573  -2.025151  -0.286253   7.275875  -3.706951   \n",
      "2   -10.585483   9.410000   4.286161   5.350992  -1.949721   2.709703   \n",
      "3    25.754696   6.026720   4.139323   0.027781  10.913109   6.300861   \n",
      "4   -10.300227   9.932291   3.530113   6.089375  -1.856193   1.085880   \n",
      "..         ...        ...        ...        ...        ...        ...   \n",
      "305  12.846466  -8.692559  -1.520787  -1.160055   4.637819   5.965863   \n",
      "306 -18.903531   5.409381   1.009240  -9.047834  -1.598162  -2.112807   \n",
      "307   7.496317 -14.345002  -5.633839  -4.835230  -5.845390  -0.117176   \n",
      "308 -18.090808   7.018423   2.736675  -8.158891  -7.647230  -1.626803   \n",
      "309 -17.005506   3.915885   1.973801  -8.465825  -1.310751   0.374418   \n",
      "\n",
      "     Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_19  Feature_20  \\\n",
      "0    -1.206552   1.038941  -2.142024   0.130237  ...   -1.625945   -1.160099   \n",
      "1    -4.659667   1.476679   2.883185   1.850452  ...    0.219722    1.703158   \n",
      "2     0.526380  -2.815417  -2.733451  -1.414169  ...    0.005490    0.394640   \n",
      "3     1.066138   7.708295  -1.060325   7.304391  ...    1.894405   -4.855668   \n",
      "4    -1.605619  -2.256021  -3.482867  -1.303468  ...    1.704122    0.729874   \n",
      "..         ...        ...        ...        ...  ...         ...         ...   \n",
      "305   2.608789  -7.046960  -0.536796  -4.446049  ...    1.377793   -0.372408   \n",
      "306  -2.327150   5.115151  -3.483121  -1.068363  ...    0.631035    0.753746   \n",
      "307  -3.593965   1.205746   1.020243   1.585316  ...   -2.169506   -0.085507   \n",
      "308   1.720418   1.432156  -2.448483   1.345725  ...    0.543133   -0.701503   \n",
      "309   0.000918   0.290243  -0.574931  -1.445133  ...    2.896289   -2.134239   \n",
      "\n",
      "     Feature_21  Feature_22  Feature_23  Feature_24  Feature_25  Feature_26  \\\n",
      "0     -0.557451   -0.336124    0.067472   -0.637111    0.394480    0.692860   \n",
      "1      0.054654   -0.035575   -1.490887   -0.344580    2.105985    1.701163   \n",
      "2      1.105900    0.691803   -1.031526   -2.135987    0.874110   -1.904065   \n",
      "3     -0.895010   -0.397437    0.676966    3.697157   -1.386428    0.002611   \n",
      "4     -0.428887    0.715349    2.105172   -0.623132    0.179648    1.095137   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "305    0.902566   -0.592298   -0.645088    0.614463    0.034115   -0.281407   \n",
      "306    2.138719    0.326552   -1.338178    0.229417   -0.238567   -0.580970   \n",
      "307   -0.041577   -1.551583    1.371924   -0.245598    0.848351   -3.265540   \n",
      "308    4.026196    1.105249    0.878779    2.256721   -2.348213    0.383610   \n",
      "309   -2.809517    0.204548    0.181115   -0.359875    1.780024    1.936950   \n",
      "\n",
      "     Feature_27  Feature_28  \n",
      "0      0.607838    0.500881  \n",
      "1     -0.348495    1.862632  \n",
      "2      0.754458    0.789843  \n",
      "3      4.581456    1.791075  \n",
      "4      2.405486    0.054774  \n",
      "..          ...         ...  \n",
      "305    0.653188    1.262555  \n",
      "306   -0.026740   -0.767655  \n",
      "307   -0.302393    0.403483  \n",
      "308   -2.678235   -0.116681  \n",
      "309   -0.795343   -0.572548  \n",
      "\n",
      "[310 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the transformed array into a DataFrame\n",
    "transformed_df = pd.DataFrame(X_train_transformed,\n",
    "                              columns=[f'Feature_{i}' for i in range(X_train_transformed.shape[1])])\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(transformed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Loadings for the top influential components:\n",
      "                value  population   value_y  value_x.2  value_CES0500000003  \\\n",
      "Component 1  0.060905    0.059212  0.053602   0.061492             0.037739   \n",
      "\n",
      "             value_y.2  value_GDPNOW  value_A191RL1Q225SBEA  \\\n",
      "Component 1   0.000544      0.002172              -0.000266   \n",
      "\n",
      "             Monthly Nominal GDP Index  Monthly Real GDP Index  ...  \\\n",
      "Component 1                   0.062057                0.059964  ...   \n",
      "\n",
      "             usinea1986  uslama4503  usinea1980  usinea1968  usinea1122  \\\n",
      "Component 1    0.004203    0.035704   -0.006708   -0.006523    0.009303   \n",
      "\n",
      "             usinea1104  usinea1113  usinea1089  usinea1092       NFP  \n",
      "Component 1    0.003292    0.000627    0.002756    0.003177  0.053602  \n",
      "\n",
      "[1 rows x 676 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Fit your model on the transformed data\n",
    "model = XGBRegressor(n_estimators=200, max_depth=20, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Compute SHAP values to identify influential components\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_train_transformed)\n",
    "\n",
    "# Assuming we're interested in the top 1 component for demonstration\n",
    "top_components = np.argsort(-np.sum(np.abs(shap_values.values), axis=0))[0:1]\n",
    "\n",
    "# Extract PCA step from the pipeline\n",
    "pca = preproc.named_steps['pca']\n",
    "\n",
    "# Extract PCA loadings for the top influential components\n",
    "loadings = pca.components_[top_components]\n",
    "\n",
    "# Assuming X_train is a DataFrame and has column names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame for loadings\n",
    "loading_df = pd.DataFrame(loadings, columns=feature_names, index=[f'Component {i+1}' for i in top_components])\n",
    "\n",
    "print(\"PCA Loadings for the top influential components:\")\n",
    "print(loading_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 PCA Loadings for Component 1 (Features): ['uslama02532', 'uslama02531', 'uslama02530', 'value_M2REAL', 'bea020_76w729rc_m', 'bea022_76w826rc_m', 'bea018_76w823rc_m', 'usinea2003', 'usinea1997', 'usinea1042', 'bea012_76a048rc_m', 'usinea1009', 'uslama02529', 'bea001_76a065rc_m', 'bea019_76w824rc_m', 'usinea1039', 'usinea1006', 'bea027_76a067rc_m', 'uslama02562', 'usinea1037', 'uslama0254', 'uslama0255', 'usinea1000', 'usinea1033', 'usinea1002', 'usinea1035', 'uslama0244', 'uslama0245', 'uslama0252', 'uslama0253', 'usinea1993', 'usinea1999', 'uslama0226', 'uslama0227', 'usinea1004', 'bea038_76a229rc_m', 'usinea1998', 'oecd_mei_00438733', 'oecd_mei_00438732', 'uslama0218', 'uslama0219', 'uslama0222', 'uslama0223', 'usinea2000', 'uslama4890', 'usinea2002', 'uslama00916', 'usinea1994', 'usinea1992', 'uslama00915']\n",
      "Top 50 PCA Loadings for Component 1 (Values): [0.06335713492990883, 0.06334114177535607, 0.06333857581834945, 0.06315393516128621, 0.06310263186209304, 0.06308512662753733, 0.06296868589621252, 0.06278917600326707, 0.0627758460533721, 0.06268715971566888, 0.06267949691242414, 0.06264166182205423, 0.06258988597665156, 0.06258856671546092, 0.06255303887754607, 0.06253447700984587, 0.06251728492368475, 0.06249651500702301, 0.06249370700443033, 0.06246793014415291, 0.062460789622366404, 0.06244880149477248, 0.06242858290023047, 0.062426656776376974, 0.062395788439040825, 0.06239484121816388, 0.06238179902992031, 0.062372977574810765, 0.06236929008220045, 0.062351602692951875, 0.0623252047184975, 0.06231847011294856, 0.062195153285847425, 0.06218852462669037, 0.06218543767237348, 0.06217140585727988, 0.06215304186258684, 0.06214573921281734, 0.06214573920895881, 0.062145602401311645, 0.06214367844845023, 0.06214328666856379, 0.06213232103610628, 0.062131157992175425, 0.06212129012361518, 0.06212127319349802, 0.06210387263928939, 0.06210310742409295, 0.06209889307396181, 0.06209403514379693]\n"
     ]
    }
   ],
   "source": [
    "# Assuming loading_df is already created and contains the PCA loadings for Component 1\n",
    "# Extract loadings for Component 1\n",
    "component_1_loadings = loading_df.loc['Component 1']\n",
    "\n",
    "# Sort the loadings by their absolute values in descending order to find the top features\n",
    "sorted_loadings = component_1_loadings.abs().sort_values(ascending=False)\n",
    "\n",
    "# Select the top 20 features based on their loadings\n",
    "top_50_features = sorted_loadings.head(50)\n",
    "\n",
    "# Convert the series to a list of feature names\n",
    "top_50_feature_names = top_50_features.index.tolist()\n",
    "\n",
    "# Optionally, to get the corresponding loading values as well\n",
    "top_50_feature_values = top_50_features.values.tolist()\n",
    "\n",
    "print(\"Top 50 PCA Loadings for Component 1 (Features):\", top_50_feature_names)\n",
    "print(\"Top 50 PCA Loadings for Component 1 (Values):\", top_50_feature_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
