{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import usual libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 1 - DATA MANIPULATION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 2 - DATA VISUALISATION\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 3 - STATISTICS\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "# 4 - MACHINE LEARNING\n",
    "## 4.1 - Preprocessing\n",
    "\n",
    "### 4.1.1 - Scalers\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "### 4.1.2 - Encoders\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "### 4.1.3 - Crossvalidation, Training, Model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "### 4.1.4 - Evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics  import ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from math import sqrt\n",
    "\n",
    "# Make all figures tiny for readability purpose\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (5,3)\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdpnow = pd.read_csv('data/gdpnow_daily_df.csv', index_col='Unnamed: 0', parse_dates=True) #date_parser=dateparse)\n",
    "# Ensure that load_df index is in the same date format\n",
    "gdpnow.index = pd.to_datetime(gdpnow.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = 'Advance Estimate From BEA'\n",
    "Drop = ['GDP Nowcast', 'Quarter being forecasted', 'Advance Estimate From BEA', 'Publication Date of Advance Estimate',\n",
    "       'Days until advance estimate', 'Forecast Error', 'Data releases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = gdpnow.drop(columns=Drop)\n",
    "y = gdpnow['Advance Estimate From BEA']\n",
    "y= y[-X.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PCE', 'PCE Goods', 'PCE Services', 'GPDI', 'Fixed Investment',\n",
       "       'Business Fixed Investment', 'Equipment',\n",
       "       'Intellectual Property Products', 'Structures', 'Residential',\n",
       "       'Government', 'Federal Govt', 'S&L', 'Imports', 'Goods imports',\n",
       "       'Services imports', 'Exports', 'Goods exports', 'Services exports',\n",
       "       'Net Exports (current, $Bil 2009)', 'Net Exports (previous, $Bil 2009)',\n",
       "       'Change in net exports ($Bil 2009)',\n",
       "       'Previous change in private inventories ($Bil 2009)',\n",
       "       'Current change in private inventories ($Bil 2009)',\n",
       "       'Change in inventory investment ($Bil 2009)', 'Final Sales',\n",
       "       'Final Sales to Domestic Purchasers',\n",
       "       'Final Sales to Private Domestic Purchasers'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.fillna(method='ffill')\n",
    "y = pd.Series(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non Linear PCA\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Updated preprocessing pipeline with Kernel PCA\n",
    "# preproc = make_pipeline(\n",
    "#     SimpleImputer(strategy='mean'),\n",
    "#     StandardScaler(),\n",
    "#     KernelPCA(n_components=None, kernel='rbf')  # Example with RBF kernel do not enter the %\n",
    "# )\n",
    "\n",
    "# PCA Adjusted preprocessing pipeline with named steps\n",
    "# preproc = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('pca', PCA(n_components=0.95, random_state=42))  # Named step 'pca'\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Adjusted preprocessing pipeline with named steps\n",
    "preproc = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', RobustScaler()),\n",
    "\n",
    "])\n",
    "\n",
    "preproc_selector = Pipeline([\n",
    "    ('preprocessing', preproc),  # Include the preprocessing steps with PCA\n",
    "    ('feature_selection', SelectPercentile(\n",
    "        mutual_info_regression,\n",
    "        percentile=90 # Keep 90% of all features\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasbowden/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 2 is smaller than n_iter=100. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   3.7s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   4.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   4.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   4.3s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   4.5s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   4.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   4.8s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   4.9s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   2.8s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   2.1s\n",
      "Best parameters found:  {'model__n_estimators': 200, 'model__max_depth': 16, 'model__learning_rate': 0.1}\n",
      "Best score found:  -1.2667528250858051\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = XGBRegressor(random_state=42)\n",
    "\n",
    "param_distributions = {\n",
    "    'model__n_estimators': [200],\n",
    "    'model__learning_rate': [0.1],\n",
    "    'model__max_depth': [16, 20],\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preproc),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,  # Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.\n",
    "    scoring='neg_mean_absolute_error',  # Assuming MSE is the metric of interest; adjust as needed.\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best score found: \", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17983943513682962\n",
      "-1.2697662951594968\n"
     ]
    }
   ],
   "source": [
    "# genrally higher N_estimators and lower learning rates are more robust\n",
    "# Best parameters found:  {'model__n_estimators': 150, 'model__max_depth': 6, 'model__learning_rate': 0.1}\n",
    "# Best score found:  -0.10133019738065827\n",
    "\n",
    "# Defining the best model\n",
    "model_best =  XGBRegressor(n_estimators=200, max_depth = 20, learning_rate = 0.1, random_state=42)\n",
    "\n",
    "# Creating the pipeline with memory caching\n",
    "pipe_best = make_pipeline(preproc_selector, model_best)\n",
    "\n",
    "# Assuming X_train and y_train are already defined and contain your training data\n",
    "score = cross_val_score(pipe_best, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Printing the standard deviation and mean of the cross-validation scores\n",
    "print(score.std())\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_best.fit(X_train,y_train)\n",
    "y_pred = pd.Series(pipe_best.predict(X_test)).reset_index(drop=True)\n",
    "y_test = pd.Series(y_test).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def forecast_accuracy(y_pred: pd.Series, y_true: pd.Series) -> float:\n",
    "\n",
    "    mape = np.mean(np.abs(y_pred - y_true)/np.abs(y_true))  # Mean Absolute Percentage Error\n",
    "    me = np.mean(y_pred - y_true)             # ME\n",
    "    mae = np.mean(np.abs(y_pred - y_true))    # MAE\n",
    "    mpe = np.mean((y_pred - y_true)/y_true)   # MPE\n",
    "    rmse = np.mean((y_pred - y_true)**2)**.5  # RMSE\n",
    "    corr = np.corrcoef(y_pred, y_true)[0,1]   # Correlation between the Actual and the Forecast\n",
    "    mins = np.amin(np.hstack([y_pred.values.reshape(-1,1), y_true.values.reshape(-1,1)]), axis=1)\n",
    "    maxs = np.amax(np.hstack([y_pred.values.reshape(-1,1), y_true.values.reshape(-1,1)]), axis=1)\n",
    "    minmax = 1 - np.mean(mins/maxs)             # minmax\n",
    "    acf1 = acf(y_pred-y_true, fft=False)[1]                      # Lag 1 Autocorrelation of Error\n",
    "\n",
    "    forecast = ({\n",
    "        'mape':mape,\n",
    "        'me':me,\n",
    "        'mae': mae,\n",
    "        'mpe': mpe,\n",
    "        'rmse':rmse,\n",
    "        'acf1':acf1,\n",
    "        'corr':corr,\n",
    "        'minmax':minmax\n",
    "    })\n",
    "\n",
    "    return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mape': 0.42859386795143084,\n",
       " 'me': 0.1604822363264447,\n",
       " 'mae': 1.120930525760247,\n",
       " 'mpe': 0.06892293250425437,\n",
       " 'rmse': 4.3908262874974255,\n",
       " 'acf1': 0.003048139746148194,\n",
       " 'corr': 0.7784853621220251,\n",
       " 'minmax': 0.3449329337055025}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_accuracy(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.100018</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.701597</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.056699</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.583259</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.090895</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>2.971240</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>2.362418</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>2.362418</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>2.362418</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>1.199886</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>508 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       y_pred  y_test\n",
       "0    1.100018     1.1\n",
       "1    0.701597     0.7\n",
       "2    3.056699     2.9\n",
       "3    2.583259     2.6\n",
       "4    2.090895     2.4\n",
       "..        ...     ...\n",
       "503  2.971240     3.0\n",
       "504  2.362418    -0.9\n",
       "505  2.362418     2.9\n",
       "506  2.362418     4.0\n",
       "507  1.199886     1.2\n",
       "\n",
       "[508 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_forecast = pd.DataFrame()\n",
    "\n",
    "# Assign rounded values of y_pred and y_test to the DataFrame\n",
    "model_forecast['y_pred'] = y_pred.values\n",
    "model_forecast['y_test'] = y_test.values\n",
    "\n",
    "# Display the DataFrame\n",
    "model_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;imputer&#x27;, KNNImputer()), (&#x27;scaler&#x27;, StandardScaler())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;imputer&#x27;, KNNImputer()), (&#x27;scaler&#x27;, StandardScaler())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNNImputer</label><div class=\"sk-toggleable__content\"><pre>KNNImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('imputer', KNNImputer()), ('scaler', StandardScaler())])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming X_train and y_train are your features and target variables respectively\n",
    "preproc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessing pipeline and transform X_train\n",
    "X_train_transformed = preproc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature_0  Feature_1     Feature_2  Feature_3     Feature_4  Feature_5  \\\n",
      "0      0.049612   0.012338  5.918930e-02  -0.016774 -1.914502e-03  -0.605582   \n",
      "1     -7.855464  -5.562308 -8.348958e+00  -5.278435 -5.455733e+00  -5.868122   \n",
      "2      0.161740  -0.030876  2.329666e-01  -0.933379 -3.926358e-01   0.131516   \n",
      "3      0.000000   0.000000 -5.936355e-17   0.000000  1.445959e-16   0.000000   \n",
      "4     -0.216694  -0.664686 -2.101560e-02   0.088988 -5.075467e-02  -0.022760   \n",
      "...         ...        ...           ...        ...           ...        ...   \n",
      "2024   0.000000   0.000000 -5.936355e-17   0.000000  1.445959e-16   0.000000   \n",
      "2025   3.553635   4.981984  2.799523e+00   0.996778  8.934885e-01  -0.451306   \n",
      "2026   0.000000   0.000000 -5.936355e-17   0.000000  1.445959e-16   0.000000   \n",
      "2027  -0.076533  -0.117305 -6.111805e-02  -0.536771 -3.763558e-01  -0.399881   \n",
      "2028   4.422632   5.630199  3.721880e+00   2.829987  1.756331e+00   0.662912   \n",
      "\n",
      "      Feature_6  Feature_7  Feature_8  Feature_9  ...    Feature_18  \\\n",
      "0     -0.365576  -0.694459  -0.899040   1.276992  ...  4.708102e-02   \n",
      "1     -5.150435  -7.047967  -3.369801  -3.132086  ... -6.974726e+00   \n",
      "2     -0.140647   0.347099   0.434868  -1.264827  ...  1.281642e-01   \n",
      "3      0.000000   0.000000   0.000000   0.000000  ...  7.201637e-17   \n",
      "4     -0.263336   0.451255   0.374236  -0.150337  ... -2.123853e-01   \n",
      "...         ...        ...        ...        ...  ...           ...   \n",
      "2024   0.000000   0.000000   0.000000   0.000000  ...  7.201637e-17   \n",
      "2025   1.096464  -2.083204  -2.748321   3.535300  ...  1.247113e+00   \n",
      "2026   0.000000   0.000000   0.000000   0.000000  ...  7.201637e-17   \n",
      "2027  -0.263336   0.104069  -0.974830  -0.248099  ...  1.281642e-01   \n",
      "2028   2.875450  -1.770736  -2.702847   3.564629  ...  1.441712e+00   \n",
      "\n",
      "        Feature_19  Feature_20    Feature_21    Feature_22    Feature_23  \\\n",
      "0     1.203355e+00    1.216724  8.047347e-02  2.766995e-01  3.159150e-01   \n",
      "1     4.420734e-01    0.193867  2.075601e+00 -1.426587e+00 -6.655463e+00   \n",
      "2    -1.338279e+00   -1.532716  1.443811e+00  1.099473e+00  8.256344e-02   \n",
      "3     4.579246e-16    0.000000  5.906764e-17 -1.025642e-16 -1.036289e-16   \n",
      "4    -2.144820e-01   -0.378933  1.277550e+00  8.973886e-01  1.190983e+00   \n",
      "...            ...         ...           ...           ...           ...   \n",
      "2024  4.579246e-16    0.000000  5.906764e-17 -1.025642e-16 -1.036289e-16   \n",
      "2025  6.998621e-01    0.337067  3.039913e+00 -5.064114e+00 -4.526130e+00   \n",
      "2026  4.579246e-16    0.000000  5.906764e-17 -1.025642e-16 -1.036289e-16   \n",
      "2027 -5.085221e-01   -0.501676 -1.190393e-01  4.643497e-01  5.339449e-02   \n",
      "2028  3.735983e-01    0.418895 -3.185520e-01 -4.919768e+00 -2.965591e+00   \n",
      "\n",
      "        Feature_24  Feature_25  Feature_26  Feature_27  \n",
      "0     5.495802e-02    0.002223    0.005164    0.057874  \n",
      "1    -5.613299e+00   -7.619494   -7.646355   -7.693464  \n",
      "2    -1.106956e+00    0.198491    0.071554    0.072694  \n",
      "3    -3.486443e-18    0.000000    0.001495   -0.000616  \n",
      "4     3.061827e-01   -0.014132   -0.127617   -0.179261  \n",
      "...            ...         ...         ...         ...  \n",
      "2024 -3.486443e-18    0.000000    0.001495   -0.000616  \n",
      "2025  6.359151e-01    3.567748    3.225109    3.140623  \n",
      "2026 -3.486443e-18    0.000000    0.001495   -0.000616  \n",
      "2027 -4.631929e-01   -0.128621   -0.127617   -0.119977  \n",
      "2028  2.158965e+00    4.156550    4.121382    4.029878  \n",
      "\n",
      "[2029 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the transformed array into a DataFrame\n",
    "transformed_df = pd.DataFrame(X_train_transformed,\n",
    "                              columns=[f'Feature_{i}' for i in range(X_train_transformed.shape[1])])\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(transformed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pca'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m top_components \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mabs(shap_values\u001b[38;5;241m.\u001b[39mvalues), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Extract PCA step from the pipeline\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m pca \u001b[38;5;241m=\u001b[39m \u001b[43mpreproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpca\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Extract PCA loadings for the top influential components\u001b[39;00m\n\u001b[1;32m     25\u001b[0m loadings \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mcomponents_[top_components]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/utils/_bunch.py:39\u001b[0m, in \u001b[0;36mBunch.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_deprecated_key_to_warnings\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}):\n\u001b[1;32m     35\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecated_key_to_warnings[key],\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m     )\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pca'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Fit your model on the transformed data\n",
    "model = XGBRegressor(n_estimators=200, max_depth=20, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Compute SHAP values to identify influential components\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_train_transformed)\n",
    "\n",
    "# Assuming we're interested in the top 1 component for demonstration\n",
    "top_components = np.argsort(-np.sum(np.abs(shap_values.values), axis=0))[0:1]\n",
    "\n",
    "# Extract PCA step from the pipeline\n",
    "pca = preproc.named_steps['pca']\n",
    "\n",
    "# Extract PCA loadings for the top influential components\n",
    "loadings = pca.components_[top_components]\n",
    "\n",
    "# Assuming X_train is a DataFrame and has column names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame for loadings\n",
    "loading_df = pd.DataFrame(loadings, columns=feature_names, index=[f'Component {i+1}' for i in top_components])\n",
    "\n",
    "print(\"PCA Loadings for the top influential components:\")\n",
    "print(loading_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming loading_df is already created and contains the PCA loadings for Component 1\n",
    "# Extract loadings for Component 1\n",
    "component_1_loadings = loading_df.loc['Component 1']\n",
    "\n",
    "# Sort the loadings by their absolute values in descending order to find the top features\n",
    "sorted_loadings = component_1_loadings.abs().sort_values(ascending=False)\n",
    "\n",
    "# Select the top 20 features based on their loadings\n",
    "top_50_features = sorted_loadings.head(50)\n",
    "\n",
    "# Convert the series to a list of feature names\n",
    "top_50_feature_names = top_50_features.index.tolist()\n",
    "\n",
    "# Optionally, to get the corresponding loading values as well\n",
    "top_50_feature_values = top_50_features.values.tolist()\n",
    "\n",
    "print(\"Top 50 PCA Loadings for Component 1 (Features):\", top_50_feature_names)\n",
    "print(\"Top 50 PCA Loadings for Component 1 (Values):\", top_50_feature_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
