{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import usual libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 1 - DATA MANIPULATION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 2 - DATA VISUALISATION\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 3 - STATISTICS\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "# 4 - MACHINE LEARNING\n",
    "## 4.1 - Preprocessing\n",
    "\n",
    "### 4.1.1 - Scalers\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "### 4.1.2 - Encoders\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "### 4.1.3 - Crossvalidation, Training, Model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "### 4.1.4 - Evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics  import ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from math import sqrt\n",
    "\n",
    "# Make all figures tiny for readability purpose\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (5,3)\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdpnow = pd.read_csv('data/gdpnow_daily_df.csv', index_col='Unnamed: 0', parse_dates=True) #date_parser=dateparse)\n",
    "# Ensure that load_df index is in the same date format\n",
    "gdpnow.index = pd.to_datetime(gdpnow.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = 'Advance Estimate From BEA'\n",
    "Drop = ['GDP Nowcast', 'Quarter being forecasted', 'Advance Estimate From BEA', 'Publication Date of Advance Estimate',\n",
    "       'Days until advance estimate', 'Forecast Error', 'Data releases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = gdpnow.drop(columns=Drop)\n",
    "y = gdpnow['Advance Estimate From BEA']\n",
    "y= y[-X.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PCE', 'PCE Goods', 'PCE Services', 'GPDI', 'Fixed Investment',\n",
       "       'Business Fixed Investment', 'Equipment',\n",
       "       'Intellectual Property Products', 'Structures', 'Residential',\n",
       "       'Government', 'Federal Govt', 'S&L', 'Imports', 'Goods imports',\n",
       "       'Services imports', 'Exports', 'Goods exports', 'Services exports',\n",
       "       'Net Exports (current, $Bil 2009)', 'Net Exports (previous, $Bil 2009)',\n",
       "       'Change in net exports ($Bil 2009)',\n",
       "       'Previous change in private inventories ($Bil 2009)',\n",
       "       'Current change in private inventories ($Bil 2009)',\n",
       "       'Change in inventory investment ($Bil 2009)', 'Final Sales',\n",
       "       'Final Sales to Domestic Purchasers',\n",
       "       'Final Sales to Private Domestic Purchasers'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.fillna(method='ffill')\n",
    "y = pd.Series(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non Linear PCA\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Updated preprocessing pipeline with Kernel PCA\n",
    "# preproc = make_pipeline(\n",
    "#     SimpleImputer(strategy='mean'),\n",
    "#     StandardScaler(),\n",
    "#     KernelPCA(n_components=None, kernel='rbf')  # Example with RBF kernel do not enter the %\n",
    "# )\n",
    "\n",
    "# PCA Adjusted preprocessing pipeline with named steps\n",
    "# preproc = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('pca', PCA(n_components=0.95, random_state=42))  # Named step 'pca'\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Adjusted preprocessing pipeline with named steps\n",
    "preproc = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', StandardScaler()),\n",
    "\n",
    "])\n",
    "\n",
    "preproc_selector = Pipeline([\n",
    "    ('preprocessing', preproc),  # Include the preprocessing steps with PCA\n",
    "    ('feature_selection', SelectPercentile(\n",
    "        mutual_info_regression,\n",
    "        percentile=90 # Keep 90% of all features\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = XGBRegressor(random_state=42)\n",
    "\n",
    "param_distributions = {\n",
    "    'model__n_estimators': [200],\n",
    "    'model__learning_rate': [0.1],\n",
    "    'model__max_depth': [16, 20],\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preproc),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,  # Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.\n",
    "    scoring='neg_mean_absolute_error',  # Assuming MSE is the metric of interest; adjust as needed.\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best score found: \", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19180801915451046\n",
      "-1.3517119762362344\n"
     ]
    }
   ],
   "source": [
    "# genrally higher N_estimators and lower learning rates are more robust\n",
    "# Best parameters found:  {'model__n_estimators': 150, 'model__max_depth': 6, 'model__learning_rate': 0.1}\n",
    "# Best score found:  -0.10133019738065827\n",
    "\n",
    "# Defining the best model\n",
    "model_best =  XGBRegressor(n_estimators=200, max_depth = 20, learning_rate = 0.1, random_state=42)\n",
    "\n",
    "# Creating the pipeline with memory caching\n",
    "pipe_best = make_pipeline(preproc_selector, model_best)\n",
    "\n",
    "# Assuming X_train and y_train are already defined and contain your training data\n",
    "score = cross_val_score(pipe_best, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Printing the standard deviation and mean of the cross-validation scores\n",
    "print(score.std())\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_best.fit(X_train,y_train)\n",
    "y_pred = pd.Series(pipe_best.predict(X_test)).reset_index(drop=True)\n",
    "y_test = pd.Series(y_test).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def forecast_accuracy(y_pred: pd.Series, y_true: pd.Series) -> float:\n",
    "\n",
    "    mape = np.mean(np.abs(y_pred - y_true)/np.abs(y_true))  # Mean Absolute Percentage Error\n",
    "    me = np.mean(y_pred - y_true)             # ME\n",
    "    mae = np.mean(np.abs(y_pred - y_true))    # MAE\n",
    "    mpe = np.mean((y_pred - y_true)/y_true)   # MPE\n",
    "    rmse = np.mean((y_pred - y_true)**2)**.5  # RMSE\n",
    "    corr = np.corrcoef(y_pred, y_true)[0,1]   # Correlation between the Actual and the Forecast\n",
    "    mins = np.amin(np.hstack([y_pred.values.reshape(-1,1), y_true.values.reshape(-1,1)]), axis=1)\n",
    "    maxs = np.amax(np.hstack([y_pred.values.reshape(-1,1), y_true.values.reshape(-1,1)]), axis=1)\n",
    "    minmax = 1 - np.mean(mins/maxs)             # minmax\n",
    "    acf1 = acf(y_pred-y_true, fft=False)[1]                      # Lag 1 Autocorrelation of Error\n",
    "\n",
    "    forecast = ({\n",
    "        'mape':mape,\n",
    "        'me':me,\n",
    "        'mae': mae,\n",
    "        'mpe': mpe,\n",
    "        'rmse':rmse,\n",
    "        'acf1':acf1,\n",
    "        'corr':corr,\n",
    "        'minmax':minmax\n",
    "    })\n",
    "\n",
    "    return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mape': 0.4593459016327415,\n",
       " 'me': -0.15542044026527826,\n",
       " 'mae': 0.8938565876950898,\n",
       " 'mpe': 0.12514118918789327,\n",
       " 'rmse': 3.227314715396498,\n",
       " 'acf1': -0.0005272892036298338,\n",
       " 'corr': 0.8996074638210155,\n",
       " 'minmax': 0.2623156347554233}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_accuracy(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.999980</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.899911</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.564316</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.186901</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.000376</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>-0.899843</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>4.064069</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.699966</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>6.499978</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>3.713404</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>508 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       y_pred  y_test\n",
       "0    3.999980     4.0\n",
       "1   -0.899911    -0.9\n",
       "2    2.564316     2.6\n",
       "3    2.186901     2.6\n",
       "4    2.000376     2.0\n",
       "..        ...     ...\n",
       "503 -0.899843    -0.9\n",
       "504  4.064069     4.1\n",
       "505  0.699966     0.7\n",
       "506  6.499978     6.5\n",
       "507  3.713404     4.0\n",
       "\n",
       "[508 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_forecast = pd.DataFrame()\n",
    "\n",
    "# Assign rounded values of y_pred and y_test to the DataFrame\n",
    "model_forecast['y_pred'] = y_pred.values\n",
    "model_forecast['y_test'] = y_test.values\n",
    "\n",
    "# Display the DataFrame\n",
    "model_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and y_train are your features and target variables respectively\n",
    "preproc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessing pipeline and transform X_train\n",
    "X_train_transformed = preproc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the transformed array into a DataFrame\n",
    "transformed_df = pd.DataFrame(X_train_transformed,\n",
    "                              columns=[f'Feature_{i}' for i in range(X_train_transformed.shape[1])])\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(transformed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Fit your model on the transformed data\n",
    "model = XGBRegressor(n_estimators=200, max_depth=20, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Compute SHAP values to identify influential components\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_train_transformed)\n",
    "\n",
    "# Assuming we're interested in the top 1 component for demonstration\n",
    "top_components = np.argsort(-np.sum(np.abs(shap_values.values), axis=0))[0:1]\n",
    "\n",
    "# Extract PCA step from the pipeline\n",
    "pca = preproc.named_steps['pca']\n",
    "\n",
    "# Extract PCA loadings for the top influential components\n",
    "loadings = pca.components_[top_components]\n",
    "\n",
    "# Assuming X_train is a DataFrame and has column names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame for loadings\n",
    "loading_df = pd.DataFrame(loadings, columns=feature_names, index=[f'Component {i+1}' for i in top_components])\n",
    "\n",
    "print(\"PCA Loadings for the top influential components:\")\n",
    "print(loading_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming loading_df is already created and contains the PCA loadings for Component 1\n",
    "# Extract loadings for Component 1\n",
    "component_1_loadings = loading_df.loc['Component 1']\n",
    "\n",
    "# Sort the loadings by their absolute values in descending order to find the top features\n",
    "sorted_loadings = component_1_loadings.abs().sort_values(ascending=False)\n",
    "\n",
    "# Select the top 20 features based on their loadings\n",
    "top_50_features = sorted_loadings.head(50)\n",
    "\n",
    "# Convert the series to a list of feature names\n",
    "top_50_feature_names = top_50_features.index.tolist()\n",
    "\n",
    "# Optionally, to get the corresponding loading values as well\n",
    "top_50_feature_values = top_50_features.values.tolist()\n",
    "\n",
    "print(\"Top 50 PCA Loadings for Component 1 (Features):\", top_50_feature_names)\n",
    "print(\"Top 50 PCA Loadings for Component 1 (Values):\", top_50_feature_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
