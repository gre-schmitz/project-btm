{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import usual libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 1 - DATA MANIPULATION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 2 - DATA VISUALISATION\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 3 - STATISTICS\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "# 4 - MACHINE LEARNING\n",
    "## 4.1 - Preprocessing\n",
    "\n",
    "### 4.1.1 - Scalers\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "### 4.1.2 - Encoders\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "### 4.1.3 - Crossvalidation, Training, Model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "### 4.1.4 - Evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics  import ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from math import sqrt\n",
    "\n",
    "# Make all figures tiny for readability purpose\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (5,3)\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdpnow = pd.read_csv('gdpnow_hf.csv', index_col='Dates', parse_dates=True) #date_parser=dateparse)\n",
    "# Ensure that load_df index is in the same date format\n",
    "gdpnow.index = pd.to_datetime(gdpnow.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = 'Final_GDP_Interp'\n",
    "Drop = ['GDP Nowcast', 'Final_GDP_Interp', 'Quarter being forecasted', 'Advance Estimate From BEA', 'Publication Date of Advance Estimate',\n",
    "       'Days until advance estimate', 'Forecast Error', 'Data releases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = gdpnow.drop(columns=Drop)\n",
    "y = gdpnow[Target]\n",
    "\n",
    "y= y[-X.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dates\n",
       "2014-05-01    0.110000\n",
       "2014-05-02    0.152667\n",
       "2014-05-05    0.280667\n",
       "2014-05-06    0.323333\n",
       "2014-05-07    0.366000\n",
       "                ...   \n",
       "2024-01-15    3.374353\n",
       "2024-01-16    3.355765\n",
       "2024-01-17    3.337176\n",
       "2024-01-18    3.318588\n",
       "2024-01-19    3.300000\n",
       "Name: Final_GDP_Interp, Length: 2537, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = y.fillna(method='ffill')\n",
    "# y = pd.Series(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non Linear PCA\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Updated preprocessing pipeline with Kernel PCA\n",
    "# preproc = make_pipeline(\n",
    "#     SimpleImputer(strategy='mean'),\n",
    "#     StandardScaler(),\n",
    "#     KernelPCA(n_components=None, kernel='rbf')  # Example with RBF kernel do not enter the %\n",
    "# )\n",
    "\n",
    "# PCA Adjusted preprocessing pipeline with named steps\n",
    "# preproc = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('pca', PCA(n_components=0.95, random_state=42))  # Named step 'pca'\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Adjusted preprocessing pipeline with named steps\n",
    "preproc = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=3)),\n",
    "    ('scaler', RobustScaler()),\n",
    "\n",
    "])\n",
    "\n",
    "preproc_selector = Pipeline([\n",
    "    ('preprocessing', preproc),  # Include the preprocessing steps with PCA\n",
    "    ('feature_selection', SelectPercentile(\n",
    "        mutual_info_regression,\n",
    "        percentile=90 # Keep 90% of all features\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasbowden/.pyenv/versions/lewagon/envs/project-btm/lib/python3.10/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 2 is smaller than n_iter=100. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   3.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   3.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   3.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   3.0s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   3.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   3.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   3.3s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   3.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   1.5s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   1.5s\n",
      "Best parameters found:  {'model__n_estimators': 200, 'model__max_depth': 20, 'model__learning_rate': 0.1}\n",
      "Best score found:  -1.1787284500750674\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = XGBRegressor(random_state=42)\n",
    "\n",
    "param_distributions = {\n",
    "    'model__n_estimators': [200],\n",
    "    'model__learning_rate': [0.1],\n",
    "    'model__max_depth': [16, 20],\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preproc),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,  # Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.\n",
    "    scoring='neg_mean_absolute_error',  # Assuming MSE is the metric of interest; adjust as needed.\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best score found: \", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05381251060833955\n",
      "-0.49307324150226267\n"
     ]
    }
   ],
   "source": [
    "# genrally higher N_estimators and lower learning rates are more robust\n",
    "# Best parameters found:  {'model__n_estimators': 150, 'model__max_depth': 6, 'model__learning_rate': 0.1}\n",
    "# Best score found:  -0.10133019738065827\n",
    "\n",
    "# Defining the best model\n",
    "model_best =  XGBRegressor(n_estimators=200, max_depth = 16, learning_rate = 0.1, random_state=42)\n",
    "\n",
    "# Creating the pipeline with memory caching\n",
    "pipe_best = make_pipeline(preproc_selector, model_best)\n",
    "\n",
    "# Assuming X_train and y_train are already defined and contain your training data\n",
    "score = cross_val_score(pipe_best, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Printing the standard deviation and mean of the cross-validation scores\n",
    "print(score.std())\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_best.fit(X_train,y_train)\n",
    "y_pred = pd.Series(pipe_best.predict(X_test)).reset_index(drop=True)\n",
    "y_test = pd.Series(y_test).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def forecast_accuracy(y_pred: pd.Series, y_true: pd.Series) -> float:\n",
    "\n",
    "    mape = np.mean(np.abs(y_pred - y_true)/np.abs(y_true))  # Mean Absolute Percentage Error\n",
    "    me = np.mean(y_pred - y_true)             # ME\n",
    "    mae = np.mean(np.abs(y_pred - y_true))    # MAE\n",
    "    mpe = np.mean((y_pred - y_true)/y_true)   # MPE\n",
    "    rmse = np.mean((y_pred - y_true)**2)**.5  # RMSE\n",
    "    corr = np.corrcoef(y_pred, y_true)[0,1]   # Correlation between the Actual and the Forecast\n",
    "    mins = np.amin(np.hstack([y_pred.values.reshape(-1,1), y_true.values.reshape(-1,1)]), axis=1)\n",
    "    maxs = np.amax(np.hstack([y_pred.values.reshape(-1,1), y_true.values.reshape(-1,1)]), axis=1)\n",
    "    minmax = 1 - np.mean(mins/maxs)             # minmax\n",
    "    acf1 = acf(y_pred-y_true, fft=False)[1]                      # Lag 1 Autocorrelation of Error\n",
    "\n",
    "    forecast = ({\n",
    "        'mape':mape,\n",
    "        'me':me,\n",
    "        'mae': mae,\n",
    "        'mpe': mpe,\n",
    "        'rmse':rmse,\n",
    "        'acf1':acf1,\n",
    "        'corr':corr,\n",
    "        'minmax':minmax\n",
    "    })\n",
    "\n",
    "    return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mape': 0.2545291284531343,\n",
       " 'me': -0.021508168415027982,\n",
       " 'mae': 0.4442398624232382,\n",
       " 'mpe': 0.13265314768656827,\n",
       " 'rmse': 1.6760666343606618,\n",
       " 'acf1': 0.03456937139116738,\n",
       " 'corr': 0.9639912498354158,\n",
       " 'minmax': 0.0665763868170659}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_accuracy(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mape': 1.051006638571981,\n",
       " 'me': 0.417513745814221,\n",
       " 'mae': 2.6060549265876,\n",
       " 'mpe': 0.36004739951206266,\n",
       " 'rmse': 6.944290781867663,\n",
       " 'acf1': 0.9898463580140768,\n",
       " 'corr': 0.5758029675582143}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline no additional data\n",
    "{'mape': 0.4963433333215978,\n",
    " 'me': 0.1525253749298041,\n",
    " 'mae': 1.0733358723767286,\n",
    " 'mpe': 0.04441432676896753,\n",
    " 'rmse': 3.325099172055762,\n",
    " 'acf1': 0.019003663322351982,\n",
    " 'corr': 0.7977906473665369,\n",
    " 'minmax': 0.21016413941493906}\n",
    "\n",
    "# Baseline with High Frequency\n",
    "{'mape': 0.2545291284531343,\n",
    " 'me': -0.021508168415027982,\n",
    " 'mae': 0.4442398624232382,\n",
    " 'mpe': 0.13265314768656827,\n",
    " 'rmse': 1.6760666343606618,\n",
    " 'acf1': 0.03456937139116738,\n",
    " 'corr': 0.9639912498354158,\n",
    " 'minmax': 0.0665763868170659}\n",
    "\n",
    "# GDP Now\n",
    "{'mape': 1.051006638571981,\n",
    " 'me': 0.417513745814221,\n",
    " 'mae': 2.6060549265876,\n",
    " 'mpe': 0.36004739951206266,\n",
    " 'rmse': 6.944290781867663,\n",
    " 'acf1': 0.9898463580140768,\n",
    " 'corr': 0.5758029675582143,}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.286994</td>\n",
       "      <td>1.268462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.931285</td>\n",
       "      <td>3.376240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.437019</td>\n",
       "      <td>6.440769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.840852</td>\n",
       "      <td>0.776957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.432255</td>\n",
       "      <td>2.478876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>2.016037</td>\n",
       "      <td>2.035208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>3.941055</td>\n",
       "      <td>5.712637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>3.131536</td>\n",
       "      <td>0.779011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>0.727237</td>\n",
       "      <td>0.820435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>1.018696</td>\n",
       "      <td>0.631778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>508 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       y_pred    y_test\n",
       "0    1.286994  1.268462\n",
       "1    2.931285  3.376240\n",
       "2    6.437019  6.440769\n",
       "3    0.840852  0.776957\n",
       "4    2.432255  2.478876\n",
       "..        ...       ...\n",
       "503  2.016037  2.035208\n",
       "504  3.941055  5.712637\n",
       "505  3.131536  0.779011\n",
       "506  0.727237  0.820435\n",
       "507  1.018696  0.631778\n",
       "\n",
       "[508 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_forecast = pd.DataFrame()\n",
    "\n",
    "# Assign rounded values of y_pred and y_test to the DataFrame\n",
    "model_forecast['y_pred'] = y_pred.values\n",
    "model_forecast['y_test'] = y_test.values\n",
    "\n",
    "# Display the DataFrame\n",
    "model_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;imputer&#x27;, KNNImputer()), (&#x27;scaler&#x27;, RobustScaler())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;imputer&#x27;, KNNImputer()), (&#x27;scaler&#x27;, RobustScaler())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNNImputer</label><div class=\"sk-toggleable__content\"><pre>KNNImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('imputer', KNNImputer()), ('scaler', RobustScaler())])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming X_train and y_train are your features and target variables respectively\n",
    "preproc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessing pipeline and transform X_train\n",
    "X_train_transformed = preproc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature_0  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  \\\n",
      "0      0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "1     -3.213498  -1.212457   6.800304   0.516424  -2.526959  -1.354394   \n",
      "2      0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "3      5.882307   8.087543  -3.088585  -7.704362  15.273041   5.077121   \n",
      "4     -1.140190   1.687543   7.578082   0.709855  -3.046959   2.837576   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "2024   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "2025   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "2026 -18.395467   1.487543   8.244748  -2.675175 -19.566959 -10.599696   \n",
      "2027   6.751759  12.037543  -7.755252  10.478083   9.313041   4.215757   \n",
      "2028  -0.337618   1.937543   1.466970   0.468067   0.273041   0.138636   \n",
      "\n",
      "      Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_18  Feature_19  \\\n",
      "0      0.000000   0.000000   0.000000    0.00000  ...    0.000000    0.000000   \n",
      "1      2.536537   1.000000  -1.709466   -2.32767  ...   -0.464189   -1.167771   \n",
      "2      0.000000   0.000000   0.000000    0.00000  ...    0.000000    0.000000   \n",
      "3      5.736537  90.017446  70.040534   77.42233  ...    0.312826   86.582229   \n",
      "4      1.936537   7.062120   2.540534    2.42233  ...   -0.240120    3.332229   \n",
      "...         ...        ...        ...        ...  ...         ...         ...   \n",
      "2024   0.000000   0.000000   0.000000    0.00000  ...    0.000000    0.000000   \n",
      "2025   0.000000   0.000000   0.000000    0.00000  ...    0.000000    0.000000   \n",
      "2026   2.736537 -65.683320 -52.459466  -62.57767  ...    0.161037  -61.167771   \n",
      "2027  10.536537  12.167063  12.040534   12.67233  ...   -0.558154    6.582229   \n",
      "2028  -5.663463  -2.509648  -2.209466   -1.32767  ...    0.894683   -0.667771   \n",
      "\n",
      "      Feature_20  Feature_21  Feature_22  Feature_23  Feature_24  Feature_25  \\\n",
      "0       0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "1      -1.440009    0.252940    1.093943    0.299942   -0.298722   -0.096575   \n",
      "2       0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "3      36.650900   79.192284  -10.053974   13.947000   25.047477   -4.702190   \n",
      "4       0.832718    3.490200   -0.597033    0.564647    1.204017    3.302807   \n",
      "...          ...         ...         ...         ...         ...         ...   \n",
      "2024    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "2025    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "2026  -14.349100  -69.970704   -2.695096   -8.553000   -0.899817  -22.466703   \n",
      "2027    1.105446    8.221580   -0.753605    8.417589    2.005478   -0.206232   \n",
      "2028   -0.167282   -0.992160   -0.910177   -2.111823   -3.103835   -0.206232   \n",
      "\n",
      "      Feature_26  Feature_27  \n",
      "0       0.000000    0.000000  \n",
      "1      -0.721707   -3.387005  \n",
      "2       0.000000    0.000000  \n",
      "3       4.478293   -4.669360  \n",
      "4      -6.121707    1.337462  \n",
      "...          ...         ...  \n",
      "2024    0.000000    0.000000  \n",
      "2025    0.000000    0.000000  \n",
      "2026  -32.621707   -5.715492  \n",
      "2027    5.778293   -1.092264  \n",
      "2028   -3.121707   -0.687310  \n",
      "\n",
      "[2029 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the transformed array into a DataFrame\n",
    "transformed_df = pd.DataFrame(X_train_transformed,\n",
    "                              columns=[f'Feature_{i}' for i in range(X_train_transformed.shape[1])])\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(transformed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBRegressor\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Fit your model on the transformed data\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Fit your model on the transformed data\n",
    "model = XGBRegressor(n_estimators=200, max_depth=20, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Compute SHAP values to identify influential components\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_train_transformed)\n",
    "\n",
    "# Assuming we're interested in the top 1 component for demonstration\n",
    "top_components = np.argsort(-np.sum(np.abs(shap_values.values), axis=0))[0:1]\n",
    "\n",
    "# Extract PCA step from the pipeline\n",
    "pca = preproc.named_steps['pca']\n",
    "\n",
    "# Extract PCA loadings for the top influential components\n",
    "loadings = pca.components_[top_components]\n",
    "\n",
    "# Assuming X_train is a DataFrame and has column names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame for loadings\n",
    "loading_df = pd.DataFrame(loadings, columns=feature_names, index=[f'Component {i+1}' for i in top_components])\n",
    "\n",
    "print(\"PCA Loadings for the top influential components:\")\n",
    "print(loading_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming loading_df is already created and contains the PCA loadings for Component 1\n",
    "# Extract loadings for Component 1\n",
    "component_1_loadings = loading_df.loc['Component 1']\n",
    "\n",
    "# Sort the loadings by their absolute values in descending order to find the top features\n",
    "sorted_loadings = component_1_loadings.abs().sort_values(ascending=False)\n",
    "\n",
    "# Select the top 20 features based on their loadings\n",
    "top_50_features = sorted_loadings.head(50)\n",
    "\n",
    "# Convert the series to a list of feature names\n",
    "top_50_feature_names = top_50_features.index.tolist()\n",
    "\n",
    "# Optionally, to get the corresponding loading values as well\n",
    "top_50_feature_values = top_50_features.values.tolist()\n",
    "\n",
    "print(\"Top 50 PCA Loadings for Component 1 (Features):\", top_50_feature_names)\n",
    "print(\"Top 50 PCA Loadings for Component 1 (Values):\", top_50_feature_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdpnow_test = gdpnow['GDP Nowcast']\n",
    "gdpnow_test = gdpnow_test.interpolate(method='time')\n",
    "gdp_test = gdpnow['Final_GDP_Interp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdpnow_test = pd.Series(gdpnow_test).reset_index(drop=True)\n",
    "gdp_test = pd.Series(gdp_test).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/5q966zvj59j8p5z9qllhcr7r0000gp/T/ipykernel_55229/2581306049.py:13: RuntimeWarning: divide by zero encountered in divide\n",
      "  minmax = 1 - np.mean(mins/maxs)             # minmax\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mape': 1.051006638571981,\n",
       " 'me': 0.417513745814221,\n",
       " 'mae': 2.6060549265876,\n",
       " 'mpe': 0.36004739951206266,\n",
       " 'rmse': 6.944290781867663,\n",
       " 'acf1': 0.9898463580140768,\n",
       " 'corr': 0.5758029675582143,\n",
       " 'minmax': inf}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_accuracy(gdpnow_test,gdp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mape': 0.3793836113654958,\n",
       " 'me': 0.050791386252946914,\n",
       " 'mae': 1.2648437748039127,\n",
       " 'mpe': 0.01996571667170425,\n",
       " 'rmse': 4.054536843585369,\n",
       " 'acf1': -0.01184796160451125,\n",
       " 'corr': 0.7459617866029101,\n",
       " 'minmax': 0.354164318228377}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'mape': 0.3793836113654958,\n",
    " 'me': 0.050791386252946914,\n",
    " 'mae': 1.2648437748039127,\n",
    " 'mpe': 0.01996571667170425,\n",
    " 'rmse': 4.054536843585369,\n",
    " 'acf1': -0.01184796160451125,\n",
    " 'corr': 0.7459617866029101,\n",
    " 'minmax': 0.354164318228377}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'mape': 1.051006638571981,\n",
    " 'me': 0.417513745814221,\n",
    " 'mae': 2.6060549265876,\n",
    " 'mpe': 0.36004739951206266,\n",
    " 'rmse': 6.944290781867663,\n",
    " 'acf1': 0.9898463580140768,\n",
    " 'corr': 0.5758029675582143,\n",
    " 'minmax': inf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
