{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdpnow = pd.read_csv('gdpnow_daily_gdp_interp.csv', index_col='Unnamed: 0', parse_dates=True) #date_parser=dateparse)\n",
    "# Ensure that load_df index is in the same date format\n",
    "gdpnow.index = pd.to_datetime(gdpnow.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = 'Final_GDP_Interp'\n",
    "Drop = ['GDP Nowcast', 'Final_GDP_Interp', 'Quarter being forecasted', 'Advance Estimate From BEA', 'Publication Date of Advance Estimate',\n",
    "       'Days until advance estimate', 'Forecast Error', 'Data releases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From here on lets try two different datasets as in how to work with the NAs:\n",
    "# 1. ffil\n",
    "# 2. dropping NAs\n",
    "\n",
    "gdpnow_filled = gdpnow.fillna(method='ffill')\n",
    "gdpnow_dropped = gdpnow.dropna(axis=0, thresh=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2537, 36), (1496, 36))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdpnow_filled.shape, gdpnow_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining X and y for both data sets (dropped and filled)\n",
    "\n",
    "X_filled = gdpnow_filled.drop(columns=Drop)\n",
    "y_filled = gdpnow_filled[Target]\n",
    "\n",
    "X_dropped = gdpnow_dropped.drop(columns=Drop)\n",
    "y_dropped = gdpnow_dropped[Target]\n",
    "\n",
    "assert(X_filled.shape[0]==y_filled.shape[0])\n",
    "assert(X_dropped.shape[0]==y_dropped.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into test and train sets\n",
    "\n",
    "X_filled_train, X_filled_test, y_filled_train, y_filled_test = \\\n",
    "    train_test_split(X_filled, y_filled, test_size=0.20)\n",
    "\n",
    "X_dropped_train, X_dropped_test, y_dropped_train, y_dropped_test = \\\n",
    "    train_test_split(X_dropped, y_dropped, test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = Pipeline([\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "preproc_selector = Pipeline([\n",
    "    ('preprocessing', preproc),  # Include the preprocessing steps with PCA\n",
    "    ('feature_selection', SelectPercentile(\n",
    "        mutual_info_regression,\n",
    "        percentile=90 # Keep 90% of all features\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/die_gregette/.pyenv/versions/3.10.6/envs/project-btm/lib/python3.10/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 2 is smaller than n_iter=100. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   2.6s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   2.7s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   2.8s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   2.8s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   2.9s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   2.9s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   2.8s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   2.9s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   1.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   1.4s\n",
      "Best parameters found:  {'model__n_estimators': 200, 'model__max_depth': 16, 'model__learning_rate': 0.1}\n",
      "Best score found:  -0.1806977482327554\n"
     ]
    }
   ],
   "source": [
    "model = XGBRegressor(random_state=42)\n",
    "\n",
    "param_distributions = {\n",
    "    'model__n_estimators': [200],\n",
    "    'model__learning_rate': [0.1],\n",
    "    'model__max_depth': [16, 20],\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preproc),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,  # Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.\n",
    "    scoring='neg_mean_absolute_error',  # Assuming MSE is the metric of interest; adjust as needed.\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "random_search.fit(X_filled_train, y_filled_train)\n",
    "\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best score found: \", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/die_gregette/.pyenv/versions/3.10.6/envs/project-btm/lib/python3.10/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 2 is smaller than n_iter=100. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   2.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   2.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   2.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   2.5s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   2.6s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=16, model__n_estimators=200; total time=   2.8s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   2.7s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   2.8s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   1.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=20, model__n_estimators=200; total time=   1.4s\n",
      "Best parameters found:  {'model__n_estimators': 200, 'model__max_depth': 16, 'model__learning_rate': 0.1}\n",
      "Best score found:  -0.3591148585471509\n"
     ]
    }
   ],
   "source": [
    "model = XGBRegressor(random_state=42)\n",
    "\n",
    "param_distributions = {\n",
    "    'model__n_estimators': [200],\n",
    "    'model__learning_rate': [0.1],\n",
    "    'model__max_depth': [16, 20],\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preproc),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,  # Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.\n",
    "    scoring='neg_mean_absolute_error',  # Assuming MSE is the metric of interest; adjust as needed.\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "random_search.fit(X_dropped_train, y_dropped_train)\n",
    "\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best score found: \", random_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-btm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
